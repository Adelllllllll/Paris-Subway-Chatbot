# üìò Documentation Data Preparation - Projet Paris-Subway-Chatbot

## 1. Contexte et Objectif
Ce module "Data Preparation" a pour but de transformer les ressources brutes du portail **PRIM (√éle-de-France Mobilit√©s)** en une **Base de Connaissances Structur√©e** exploitable par un Agent IA.

L'objectif est de permettre √† l'agent de g√©n√©rer du code Python d'appel API sans hallucinations, en lui fournissant :
1.  **Le contexte m√©tier** (Documentation textuelle).
2.  **La d√©finition technique** (Swagger OpenAPI).
3.  **La structure des donn√©es** (Sch√©mas des CSV).

---

## 2. üì• Installation & R√©cup√©ration des Donn√©es

Ce d√©p√¥t ne contient pas les donn√©es CSV brutes (fichiers volumineux) pour rester l√©ger et performant.

### 1. Cloner le projet
```bash
git clone https://github.com/Adelllllllll/Paris-Subway-Chatbot.git
cd Paris-Subway-Chatbot
```

### 2. T√©l√©charger les donn√©es brutes (Datasets)
Les fichiers CSV lourds sont stock√©s externement.
üëâ [**T√©l√©charger l'archive 1_raw_data.zip**](https://drive.google.com/drive/u/5/folders/12xumsjusEErf3lzNyObJuMtXUU20cGi-)

### 3. Installer les donn√©es
D√©compressez l'archive dans le dossier `data/` du projet.
Assurez-vous d'avoir l'arborescence suivante :
```text
data/
‚îî‚îÄ‚îÄ 1_raw_data/       <-- Dossier d√©compress√©
    ‚îú‚îÄ‚îÄ datasets/
    ‚îú‚îÄ‚îÄ html/
    ‚îî‚îÄ‚îÄ schemas_bruts/
```

### 4. Installer les d√©pendances Python
```bash
pip install -r requirements.txt
```

---

## 3. Architecture des Donn√©es

Les donn√©es sont organis√©es selon un pipeline de raffinement en 3 √©tapes :

```text
data/
‚îú‚îÄ‚îÄ 1_raw_data/              # [ENTR√âE] Donn√©es brutes (Non versionn√©es sur Git)
‚îÇ   ‚îú‚îÄ‚îÄ datasets/            # Fichiers CSV r√©els (ex: arrets.csv)
‚îÇ   ‚îú‚îÄ‚îÄ html/                # Pages web de documentation (sauvegarde)
‚îÇ   ‚îî‚îÄ‚îÄ schemas_bruts/       # JSON complexes copi√©s du site (avant nettoyage)
‚îÇ
‚îú‚îÄ‚îÄ 2_clean_markdown/        # [TRAITEMENT] Documentation textuelle "LLM-Ready"
‚îÇ   ‚îî‚îÄ‚îÄ [Categorie]__[Titre].md  # HTML converti en Markdown propre
‚îÇ
‚îî‚îÄ‚îÄ 3_metadata/              # [SORTIE] Le "Cerveau" de l'Agent (Versionn√© sur Git)
    ‚îú‚îÄ‚îÄ swagger/             # D√©finitions API (fichiers .json) <-- D√âPLAC√â ICI
    ‚îú‚îÄ‚îÄ knowledge_index.json # Index liant Documentation <-> Swagger
    ‚îú‚îÄ‚îÄ datasets&api_catalogue.csv # Catalogue nettoy√© pour le choix des sources
    ‚îî‚îÄ‚îÄ schemas/             # Description simplifi√©e des colonnes CSV (.json/.md)
```

---

## 4. Pipeline de Traitement (D√©tail des √©tapes)

### √âtape 1 : Web Scraping & Nettoyage (HTML -> Markdown)
* **Source :** Pages de documentation "Guide du d√©veloppeur" du site PRIM (URLs d√©finies dans `Titres&Liens.md`).
* **Probl√®me :** Le HTML contient trop de bruit (menus, footers) et consomme trop de tokens.
* **Solution :**
    * Extraction cibl√©e de la `div` de contenu via `BeautifulSoup`.
    * Conversion en **Markdown** (format dense et structur√©).
    * Nommage des fichiers avec pr√©fixe de cat√©gorie pour le tri.
* **R√©sultat :** Dossier `2_clean_markdown/`.

### √âtape 2 : Catalogage (Le Menu)
* **Source :** Le fichier `datasets&api_catalogue.csv` original.
* **Action :** Nettoyage pour ne conserver que les **datasets** et **API** r√©ellement utilis√©s dans le projet. Suppression des colonnes inutiles ("attributs", emails vides).
* **Usage :** C'est le premier fichier que l'Agent lit pour r√©pondre √† la question "Quelles donn√©es sont disponibles ?".

### √âtape 3 : Indexation (Le Cerveau RAG)
* **Fichier cl√© :** `3_metadata/knowledge_index.json`.
* **R√¥le :** Cr√©er le lien logique entre une documentation "M√©tier" et un fichier "Technique".
* **Structure :**
    ```json
    {
      "doc_id": "doc_002",
      "doc_title": "Identification des Objets",
      "filename": "identification_des_objets.md",
      "related_technical_assets": [
          { "type": "api_definition", "filename": "swagger_prochains_passages.json" }
      ]
    }
    ```
* **M√©thode :** Indexation manuelle pour garantir une pr√©cision absolue et √©viter que l'IA ne m√©lange les API (ex: confondre l'API Temps R√©el avec le Calculateur).

### √âtape 4 : Simplification des Sch√©mas (Les Ingr√©dients)
* **Probl√®me :** Les fichiers CSV sont trop volumineux pour √™tre lus en entier par le LLM. Les d√©finitions JSON brutes du site sont trop verbeuses (GeoJSON standard).
* **Solution :** Cr√©ation de "Cartes d'identit√©" l√©g√®res pour chaque dataset.
* **Script :** `src/clean_schemas.py`.
* **R√©sultat :** Fichiers dans `3_metadata/schemas/` ne contenant que `Nom Colonne : Description`.
    * *Exemple :* `{"route_id": "Identifiant ligne", "stop_name": "Nom arr√™t"}`.

---

## 5. Mode d'Emploi pour l'Agent (Workflow Cible)

L'architecture est con√ßue pour une **Logique Agentique** en 3 temps :

1.  **S√âLECTION (Router) :**
    * L'agent re√ßoit la question.
    * Il consulte `datasets&api_catalogue.csv`.
    * Il d√©cide s'il a besoin d'un **Dataset** (ex: `arrets-lignes`) ou d'une **API** (ex: `Prochains Passages`).

2.  **R√âCUP√âRATION (Retrieval) :**
    * *Si API :* Il consulte `knowledge_index.json` pour charger le Markdown explicatif et le Swagger associ√©.
    * *Si Dataset :* Il charge le sch√©ma simplifi√© depuis `3_metadata/schemas/` pour conna√Ætre les noms de colonnes.

3.  **EX√âCUTION (Generation) :**
    * Il g√©n√®re le code Python en utilisant les bons IDs (trouv√©s via recherche dans les datasets) et les bons endpoints (trouv√©s dans le Swagger).

---

## 6. Inventaire des Scripts de Pr√©paration

Ces scripts (situ√©s dans `src/`) ne servent qu'√† la **construction** de la base de donn√©es. Ils ne sont pas utilis√©s lors de l'ex√©cution du chatbot.

| Script | Fonction |
| :--- | :--- |
| `main.py` | Orchestrateur principal qui lance le scraping des URLs d√©finies. |
| `cleaner.py` | Module de nettoyage HTML et conversion Markdown. |
| `clean_catalogue.py` | (Utilitaire) Filtre le CSV catalogue pour garder uniquement les lignes utiles. |
| `clean_schemas.py` | Transforme les JSON bruts (PIRM) en sch√©mas simplifi√©s pour le LLM. |
| `generate_metadata.py` | (Obsol√®te) Script de tentative d'indexation automatique (remplac√© par manuel). |

---

## Auteur

**Adel ZAIRI**

*Derni√®re mise √† jour : D√©cembre 2025*

